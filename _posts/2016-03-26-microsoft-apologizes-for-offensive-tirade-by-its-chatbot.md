---
ID: 2193
post_title: 'Microsoft apologizes for offensive tirade by its &#039;chatbot&#039;'
author: Staff Writer
post_date: 2016-03-26 14:21:16
post_excerpt: ""
layout: post
permalink: >
  https://www.whenitson.com/microsoft-apologizes-for-offensive-tirade-by-its-chatbot/
published: true
original_cats:
  - technologyNews
original_title:
  - 'Microsoft apologizes for offensive tirade by its &#039;chatbot&#039;'
original_link:
  - >
    http://feeds.reuters.com/~r/reuters/technologyNews/~3/BwdCp79EEU4/story01.htm
canonical_url:
  - >
    http://feeds.reuters.com/~r/reuters/technologyNews/~3/BwdCp79EEU4/story01.htm
---
 [ad_1]
<br><div id="articleText">
<span id="midArticle_start"/>

<span id="midArticle_0"/><span class="focusParagraph" readability="6"><p><span class="articleLocation">LOS ANGELES</span> Microsoft is "deeply sorry" for the racist and sexist Twitter messages generated by the so-called chatbot it launched this week, a company official wrote on Friday, after the artificial intelligence program went on an embarrassing tirade.</p></span><span id="midArticle_1"/><p>The bot, known as Tay, was designed to become "smarter" as more users interacted with it. Instead, it quickly learned to parrot a slew of anti-Semitic and other hateful invective that human Twitter users started feeding the program, forcing Microsoft Corp (<span id="symbol_MSFT.O_0">MSFT.O</span>) to shut it down on Thursday.</p><span id="midArticle_2"/><p>Following the setback, Microsoft said in a blog post it would revive Tay only if its engineers could find a way to prevent Web users from influencing the chatbot in ways that undermine the company's principles and values.</p><span id="midArticle_3"/><p>"We are deeply sorry for the unintended offensive and hurtful tweets from Tay, which do not represent who we are or what we stand for, nor how we designed Tay," wrote Peter Lee, Microsoft's vice president of research. </p><span id="midArticle_4"/><p>Microsoft created Tay as an experiment to learn more about how artificial intelligence programs can engage with Web users in casual conversation. The project was designed to interact with and "learn" from the young generation of millennials.</p><span id="midArticle_5"/>
        
        <span class="first-article-divide"/><p>Tay began its short-lived Twitter tenure on Wednesday with a handful of innocuous tweets. </p><span id="midArticle_6"/><p>Then its posts took a dark turn.</p><span id="midArticle_7"/><p>In one typical example, Tay tweeted: "feminism is cancer," in response to another Twitter user who had posted the same message.</p><span id="midArticle_8"/>
        
        <span class="second-article-divide"/><p>Lee, in the blog post, called Web users' efforts to exert a malicious influence on the chatbot "a coordinated attack by a subset of people."  </p><span id="midArticle_9"/><p>"Although we had prepared for many types of abuses of the system, we had made a critical oversight for this specific attack," Lee wrote. "As a result, Tay tweeted wildly inappropriate and reprehensible words and images."</p><span id="midArticle_10"/>
        
        <span class="third-article-divide"/><p>Microsoft has enjoyed better success with a chatbot called XiaoIce that the company launched in China in 2014. XiaoIce is used by about 40 million people and is known for "delighting with its stories and conversations," according to Microsoft.</p><span id="midArticle_11"/><p>As for Tay? Not so much.  </p><span id="midArticle_12"/><p>"We will remain steadfast in our efforts to learn from this and other experiences as we work toward contributing to an Internet that represents the best, not the worst, of humanity," Lee wrote.</p><span id="midArticle_13"/><span id="midArticle_14"/><p> (Editing by Frank McGurty and Peter Cooney)</p><span id="midArticle_15"/></div>
<br>[ad_2]
<br><a href="http://feeds.reuters.com/~r/reuters/technologyNews/~3/BwdCp79EEU4/story01.htm">Source </a>